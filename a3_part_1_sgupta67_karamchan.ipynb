{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4d8c9b006ada45",
   "metadata": {},
   "source": [
    "## <center>CSE 546: Reinforcement Learning</center>\n",
    "### <center>Prof. Alina Vereshchaka</center>\n",
    "#### <center>Spring 2025</center>\n",
    "\n",
    "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e9514",
   "metadata": {},
   "source": [
    "<b>Submitted By:</b><br>\n",
    "Name - Shivansh Gupta<br>\n",
    "UBIT No - 50604127<br>\n",
    "UBIT Name - sgupta67<br>\n",
    "UB Email ID - sgupta67@buffalo.edu<br>\n",
    "\n",
    "Name - Karan Ramchandani<br>\n",
    "UBIT No - 50610533<br>\n",
    "UBIT Name - karamchan<br>\n",
    "UB Email ID - karamchan@buffalo.edu<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a6d891e2fb312",
   "metadata": {},
   "source": [
    "## Section 0: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53473293aa9daf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112001b70>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "import ale_py\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d9c34ff222994",
   "metadata": {},
   "source": [
    "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
    "\n",
    "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
    "- A. Completely separate actor and critic networks\n",
    "- B. A shared network with two output heads\n",
    "\n",
    "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fa7887dd4f858",
   "metadata": {},
   "source": [
    "### Task 1a â€“ Separate Actor and Critic Networks with Loss Function\n",
    "\n",
    "Define a class `SeparateActorCritic`. Your goal is to:\n",
    "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
    "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
    "- The critic should output a single scalar value.\n",
    "\n",
    " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
    "\n",
    "```python\n",
    "# TODO: Define SeparateActorCritic class\n",
    "```\n",
    "\n",
    " Next, simulate training using dummy tensors:\n",
    "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
    "2. Compute the actor loss using the advantage (return - value).\n",
    "3. Compute the critic loss as mean squared error between values and returns.\n",
    "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
    "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
    "\n",
    "```python\n",
    "# TODO: Simulate loss computation and backpropagation\n",
    "```\n",
    "\n",
    "ðŸ”— Helpful references:\n",
    "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
    "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd6b81ed1791e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Loss: -0.41221699118614197\n",
      "Critic Loss: 2.0711171627044678\n",
      "Combined Loss: 1.6589001417160034\n",
      "\n",
      "Combined Optimizer Step Performed....\n",
      "\n",
      "Seperate Optimizer Step Performed....\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define a class SeparateActorCritic with separate networks for actor and critic\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "# Creating a completely independent actor-critic neural network architecture\n",
    "class SeparateActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(SeparateActorCritic, self).__init__()\n",
    "        \n",
    "        # Actor network\n",
    "        self.a_fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.a_fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "        self.a_softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Critic network\n",
    "        self.c_fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.c_fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # Forward pass for actor\n",
    "        f_actor = F.relu(self.a_fc1(state))\n",
    "        logits_actor = self.a_fc2(f_actor)\n",
    "        action_probs = self.a_softmax(logits_actor)\n",
    "        \n",
    "        # Forward pass for critic\n",
    "        f_critic = F.relu(self.c_fc1(state))\n",
    "        state_value = self.c_fc2(f_critic)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    batch_size = 5\n",
    "    state_dim = 4\n",
    "    action_dim = 2\n",
    "    hidden_dim = 128\n",
    "    dummy_states = torch.randn(batch_size, state_dim)\n",
    "    \n",
    "    # Initialize the actor-critic model\n",
    "    AC_model = SeparateActorCritic(state_dim, action_dim, hidden_dim)\n",
    "    \n",
    "    # Forward pass through the model to get the action probabilities and state values\n",
    "    action_probs, state_values = AC_model(dummy_states)\n",
    "    \n",
    "    # For simulation of loss computation, by using multinomial sampling we sampled an action for each state\n",
    "    dummy_actions = action_probs.multinomial(num_samples=1).squeeze(-1)\n",
    "    \n",
    "    # Computing the log probabilities of the dummy actions taken\n",
    "    dummy_log_probs = torch.log(action_probs[range(batch_size), dummy_actions])\n",
    "    \n",
    "    # Computing the entropy of the each action probability distribution\n",
    "    dummy_entropy_value = -torch.sum(action_probs * torch.log(action_probs + 1e-10), dim=1)\n",
    "    \n",
    "    # Creating dummy rewards (or target values) for the critic evaluation\n",
    "    dummy_critic_rewards = torch.randn(batch_size)\n",
    "    \n",
    "    # Computing the advantage estimates which is the difference between the returns and the estimated state values\n",
    "    compute_advantage = dummy_critic_rewards - state_values.squeeze(-1)\n",
    "    \n",
    "    # Computing the actor loss using the max log probabilities and the advantage estimates\n",
    "    actor_loss = -torch.mean(dummy_log_probs * compute_advantage) - 0.01 * torch.mean(dummy_entropy_value)\n",
    "    \n",
    "    # Computing the critic loss using the mean squared error between the returns and the estimated state values\n",
    "    critic_loss = F.mse_loss(state_values.squeeze(-1), dummy_critic_rewards)\n",
    "    \n",
    "    print(\"Actor Loss:\", actor_loss.item())\n",
    "    print(\"Critic Loss:\", critic_loss.item())\n",
    "    \n",
    "    # Combining and using a simple optimizer for both actor and critic\n",
    "    combined_optimizer = optim.Adam(AC_model.parameters(), lr=0.001)\n",
    "    combined_loss_computed = actor_loss + critic_loss\n",
    "    \n",
    "    # Performing a backward pass and optimizing the model using the combined loss\n",
    "    combined_optimizer.zero_grad()\n",
    "    combined_loss_computed.backward()\n",
    "    combined_optimizer.step()\n",
    "    print(\"Combined Loss:\", combined_loss_computed.item())\n",
    "    \n",
    "    print(\"\\nCombined Optimizer Step Performed....\")\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------\"\"\"\n",
    "    \n",
    "    # Using seperate optimizers for actor and critic\n",
    "    parameter_Actor = list(AC_model.a_fc1.parameters()) + list(AC_model.a_fc2.parameters())\n",
    "    parameter_Critic = list(AC_model.c_fc1.parameters()) + list(AC_model.c_fc2.parameters())\n",
    "    \n",
    "    optimizer_actor = optim.Adam(parameter_Actor, lr=0.001)\n",
    "    optimizer_critic = optim.Adam(parameter_Critic, lr=0.001)\n",
    "    \n",
    "    # Forward pass again to get a fresh computation graph as we first performed a backward pass using a combined optimizer\n",
    "    action_probs, state_values = AC_model(dummy_states)\n",
    "\n",
    "    # Recomputing dummy actions, log-probabilities, and entropies\n",
    "    dummy_actions = action_probs.multinomial(num_samples=1).squeeze(-1)\n",
    "    dummy_log_probs = torch.log(action_probs[range(batch_size), dummy_actions])\n",
    "    dummy_entropy_value = -torch.sum(action_probs * torch.log(action_probs + 1e-10), dim=1)\n",
    "\n",
    "    # Recomputing the advantage and losses for the above computed values\n",
    "    compute_advantage = dummy_critic_rewards - state_values.squeeze(-1)\n",
    "    actor_loss = -torch.mean(dummy_log_probs * compute_advantage) - 0.01 * torch.mean(dummy_entropy_value)\n",
    "    critic_loss = F.mse_loss(state_values.squeeze(-1), dummy_critic_rewards)\n",
    "\n",
    "    # Now performing a backward pass and optimizing the model using the seperate optimizers\n",
    "    optimizer_actor.zero_grad()\n",
    "    optimizer_critic.zero_grad()\n",
    "\n",
    "    torch.autograd.backward(\n",
    "        [actor_loss, critic_loss],\n",
    "        [torch.ones_like(actor_loss), torch.ones_like(critic_loss)]\n",
    "    )\n",
    "\n",
    "    optimizer_actor.step()\n",
    "    optimizer_critic.step()\n",
    "    \n",
    "    print(\"\\nSeperate Optimizer Step Performed....\")\n",
    "    \n",
    "        \n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e90c88108cd2e",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98ea382314354335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason for using separate networks for the actor and the critic is based on the requirement to maintain the independent nature of learning for both modules with regards to policy learning (actor) and value estimation (critic). The seperate nature of the two modules confirms that both networks can focus on their respective duties of policy improvement for the actor and value function approximation for the criticâ€”without interfering with each other. This seperation encourages more flexible learning dynamics, especially in situations where the value function and policy representations are very different.\n",
    "\n",
    "# The use of this kind of setup are particularly beneficial in environments where the value function and policy have conflicting or diverging learning patterns. For example, in Atari environments such as PongNoFrameskip-v4, the actor can learn in a better way by capturing fast-moving dynamics for selecting actions, while the critic could need to focus on more stable value evaluation for the long-term. Environments like HalfCheetah-v3,the actor and the critic could learning effectively by having independent model paths that successfully manage to capture different dimensions of movement and reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64081a606b93029d",
   "metadata": {},
   "source": [
    "### Task 1b â€“ Shared Network with Actor and Critic Heads + Loss Function\n",
    "\n",
    "Now define a class `SharedActorCritic`:\n",
    "- Build a shared base network (e.g., linear layer + ReLU)\n",
    "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
    "\n",
    "```python\n",
    "# TODO: Define SharedActorCritic class\n",
    "```\n",
    "\n",
    "Then:\n",
    "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
    "2. Simulate dummy rewards and compute advantage.\n",
    "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
    "\n",
    "```python\n",
    "# TODO: Simulate shared network loss computation and backpropagation\n",
    "```\n",
    "\n",
    " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
    "\n",
    "ðŸ”— More reading:\n",
    "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
    "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
    "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a48f882fff11aecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Loss (Shared): -0.1714530736207962\n",
      "Critic Loss (Shared): 0.45052018761634827\n",
      "Total Loss (Shared): 0.27906709909439087\n",
      "Shared Optimizer Step Performed....\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "# Creating a shared base network for both actor and critic\n",
    "class SharedActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(SharedActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared base network\n",
    "        self.shared_fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        \n",
    "        # Seoerate heads for actor and critic\n",
    "        self.actor_head = nn.Linear(hidden_dim, action_dim)\n",
    "        self.softmax_actor = nn.Softmax(dim=-1)\n",
    "        self.critic_head = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        shared_output = F.relu(self.shared_fc1(state))\n",
    "        \n",
    "        # Forward pass for actor\n",
    "        actor_logits = self.actor_head(shared_output)\n",
    "        probab_action = self.softmax_actor(actor_logits)\n",
    "        \n",
    "        # Forward pass for critic\n",
    "        state_value = self.critic_head(shared_output)\n",
    "        \n",
    "        return probab_action, state_value\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 5\n",
    "    state_dim = 4\n",
    "    action_dim = 2\n",
    "    hidden_dim = 128\n",
    "    dummy_states_shared = torch.randn(batch_size, state_dim)\n",
    "    \n",
    "    # Initializing the actor-critic model\n",
    "    AC_model_shared = SharedActorCritic(state_dim, action_dim, hidden_dim)\n",
    "    \n",
    "    # Forward pass through the model to get the action probabilities and state values\n",
    "    action_probs_shared, state_values_shared = AC_model_shared(dummy_states)\n",
    "    \n",
    "    # For simulation of loss computation, by using multinomial sampling we sampled an action for each state\n",
    "    dummy_actions_shared = action_probs_shared.multinomial(num_samples=1).squeeze(-1)\n",
    "    \n",
    "    # Computing the log probabilities of the dummy actions taken\n",
    "    dummy_log_probs_shared = torch.log(action_probs_shared[range(batch_size), dummy_actions_shared])\n",
    "    \n",
    "    # Computing the entropy of the each action probability distribution\n",
    "    dummy_entropy_value_shared = -torch.sum(action_probs_shared * torch.log(action_probs_shared + 1e-10), dim=1)\n",
    "    \n",
    "    # Creating dummy rewards (or target values) for the critic evaluation\n",
    "    dummy_critic_rewards_shared = torch.randn(batch_size)\n",
    "    \n",
    "    # Computing the advantage estimates which is the difference between the returns and the estimated state values\n",
    "    compute_advantage_shared = dummy_critic_rewards_shared - state_values_shared.squeeze(-1)\n",
    "    \n",
    "    # Computing the actor loss using the max log probabilities and the advantage estimates\n",
    "    actor_loss_shared = -torch.mean(dummy_log_probs_shared * compute_advantage_shared) - 0.01 * torch.mean(dummy_entropy_value_shared)\n",
    "    \n",
    "    # Computing the critic loss using the mean squared error between the returns and the estimated state values\n",
    "    critic_loss_shared = F.mse_loss(state_values_shared.squeeze(-1), dummy_critic_rewards_shared)\n",
    "    \n",
    "    print(\"Actor Loss (Shared):\", actor_loss_shared.item())\n",
    "    print(\"Critic Loss (Shared):\", critic_loss_shared.item())\n",
    "    \n",
    "    total_loss_shared = actor_loss_shared + critic_loss_shared\n",
    "    print(\"Total Loss (Shared):\", total_loss_shared.item())\n",
    "    \n",
    "    # Using a single optimizer for the shared model\n",
    "    optimizer_shared = optim.Adam(AC_model_shared.parameters(), lr=0.001)\n",
    "    \n",
    "    # Performing a backward pass and optimizing the model using the shared optimizer\n",
    "    optimizer_shared.zero_grad()\n",
    "    total_loss_shared.backward()\n",
    "    optimizer_shared.step()\n",
    "    \n",
    "    print(\"Shared Optimizer Step Performed....\")\n",
    "    \n",
    "    \n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974e302d1fdb028",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8fad5ea9406f8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The motivation behin using a shared base network for both actor and critic is to improve learning efficiency. In this setup, the actor and critic share the same layers to extract features, by following this process they donâ€™t have to learn different sets of features. The shared layers capture general patterns from input states, which the actor and critic then use for their respective outputs and this ultimately leads to fewer parameters, allowing faster learning. This is highly crucial as shared architecture simplifies the system, making it easier to scale and manage.\n",
    "\n",
    "# The use of this kind of setup are particularly beneficial in environments when both the actor and critic benefit from using the same information from the environment, and this occur when the poilicy and value function rely on somewhat same features. For example, in environments like CartPole-v1, the actor and critic can learn effectively by having a shared base network that captures the same features like cart position, angle and velocity. Similarly, in environments like LunarLander-v3, the actor and critic can learn effectively by having a shared base network that captures the same features like lander position and velocity which can help take better decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb645eb009b85b1c",
   "metadata": {},
   "source": [
    "## Section 2: Auto-Adaptive Network Setup for Environments\n",
    "\n",
    "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223b6ddf43abee5",
   "metadata": {},
   "source": [
    "### Task 2: Auto-generate Input and Output Layers\n",
    "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
    "- The input layer should match the environment's observation space.\n",
    "- The output layer for the **actor** should depend on the action space:\n",
    "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
    "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
    "- The **critic** always outputs a single scalar value.\n",
    "\n",
    "```python\n",
    "# TODO: Define function `create_shared_network(env)`\n",
    "```\n",
    "\n",
    "#### Environments to Support:\n",
    "Test your function with the following environments:\n",
    "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
    "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
    "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
    "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
    "\n",
    "```python\n",
    "# TODO: Loop through environments and test `create_shared_network`\n",
    "```\n",
    "\n",
    "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
    "\n",
    "ðŸ”— Observation/Action Space Docs:\n",
    "- https://gymnasium.farama.org/api/spaces/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6d249ff9277403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Environment: CliffWalking-v0 imported successfully-----\n",
      "\n",
      "Action Probabilities: tensor([[0.2648, 0.2445, 0.2479, 0.2428],\n",
      "        [0.2665, 0.2409, 0.2354, 0.2572]], grad_fn=<SoftmaxBackward0>), \n",
      "State Value Estimates: tensor([[-0.0416],\n",
      "        [ 0.0014]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "-----Environment: LunarLander-v3 imported successfully-----\n",
      "\n",
      "Action Probabilities: tensor([[0.1729, 0.3826, 0.2302, 0.2143],\n",
      "        [0.2624, 0.1999, 0.2749, 0.2629]], grad_fn=<SoftmaxBackward0>), \n",
      "State Value Estimates: tensor([[0.1171],\n",
      "        [0.1530]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "-----Environment: PongNoFrameskip-v4 imported successfully-----\n",
      "\n",
      "Action Probabilities: tensor([[0.1869, 0.1190, 0.1025, 0.1307, 0.1793, 0.2816],\n",
      "        [0.1485, 0.1148, 0.2463, 0.1504, 0.1575, 0.1825]],\n",
      "       grad_fn=<SoftmaxBackward0>), \n",
      "State Value Estimates: tensor([[0.0064],\n",
      "        [0.4165]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "-----Environment: HalfCheetah-v5 imported successfully-----\n",
      "\n",
      "Action Mean: tensor([[-0.3002, -0.0598, -0.2341, -0.0953, -0.1373, -0.2533],\n",
      "        [-0.3965, -0.1072, -0.4003,  0.0690, -0.1600, -0.1438]],\n",
      "       grad_fn=<AddmmBackward0>), \n",
      "Action Log Std: tensor([1., 1., 1., 1., 1., 1.], grad_fn=<ExpBackward0>), \n",
      "State Value Estimates: tensor([[ 0.1172],\n",
      "        [-0.1172]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_YOUR_CODE\n",
    "def create_shared_network(env, hidden_dim):\n",
    "    observation_space = env.observation_space\n",
    "    action_space = env.action_space\n",
    "    \n",
    "    # Finding the input dimension based on the observation space\n",
    "    if isinstance(observation_space, gym.spaces.Discrete):\n",
    "        input_dimension = observation_space.n\n",
    "        is_discrete_observation = True\n",
    "    elif isinstance(observation_space, gym.spaces.Box):\n",
    "        input_dimension = int(np.prod(observation_space.shape))\n",
    "        is_discrete_observation = False\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space type\".format(type(observation_space)))\n",
    "    \n",
    "    # Finding the actor head setting based on the action space\n",
    "    continuous_action = False\n",
    "    if isinstance(action_space, gym.spaces.Discrete):\n",
    "        actor_output_dimension = action_space.n\n",
    "    elif isinstance(action_space, gym.spaces.Box):\n",
    "        actor_output_dimension = int(np.prod(action_space.shape))\n",
    "        continuous_action = True\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported action space type\".format(type(action_space)))\n",
    "    \n",
    "    # Creating the shared actor-critic model\n",
    "    class Shared_AC_Netwok(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Shared_AC_Netwok, self).__init__()\n",
    "            self.is_discrete_observation = is_discrete_observation\n",
    "            self.continuous_action = continuous_action\n",
    "            self.input_dimension = input_dimension\n",
    "            \n",
    "            # Shared base network for actor and critic\n",
    "            self.shared_fc1 = nn.Linear(input_dimension, hidden_dim)\n",
    "            self.relu_layer = nn.ReLU()\n",
    "            \n",
    "            # Actor head configuration\n",
    "            if self.continuous_action:\n",
    "                self.actor_head_avg = nn.Linear(hidden_dim, actor_output_dimension)\n",
    "                self.actor_log_std = nn.Parameter(torch.zeros(actor_output_dimension))\n",
    "            else:\n",
    "                self.actor_head = nn.Linear(hidden_dim, actor_output_dimension)\n",
    "                self.actor_softmax = nn.Softmax(dim=-1)\n",
    "                \n",
    "            # Critic head configuration\n",
    "            self.critic_head = nn.Linear(hidden_dim, 1)\n",
    "            \n",
    "        def forward(self, state):\n",
    "            if self.is_discrete_observation:\n",
    "                state = F.one_hot(state.long(), num_classes=self.input_dimension).float()\n",
    "            else:\n",
    "                if len(state.shape) > 2:\n",
    "                    state = state.view(state.size(0), -1)\n",
    "                    \n",
    "            state = self.relu_layer(self.shared_fc1(state))\n",
    "            state_value = self.critic_head(state)\n",
    "            \n",
    "            # Computing the action probabilities for continuous and discrete action spaces\n",
    "            if self.continuous_action:\n",
    "                mean = self.actor_head_avg(state)\n",
    "                log_std = torch.exp(self.actor_log_std)\n",
    "                return (mean, log_std), state_value\n",
    "            else:\n",
    "                logits = self.actor_head(state)\n",
    "                action_probs = self.actor_softmax(logits)\n",
    "                return action_probs, state_value\n",
    "            \n",
    "    return Shared_AC_Netwok()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    used_env_names = [\n",
    "        \"CliffWalking-v0\",\n",
    "        \"LunarLander-v3\",\n",
    "        \"PongNoFrameskip-v4\",\n",
    "        \"HalfCheetah-v5\"\n",
    "    ]\n",
    "    \n",
    "    for env_name in used_env_names:\n",
    "        print(f\"\\n-----Environment: {env_name} imported successfully-----\\n\")\n",
    "        try:\n",
    "            test_env = gym.make(env_name)\n",
    "        except gym.error.NameNotFound as e:\n",
    "            print(f\"Error creating environment {env_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Apply Atari Wrappers for preprocessing for Atari pong\n",
    "        if env_name == \"PongNoFrameskip-v4\":\n",
    "            try:\n",
    "                test_env = gym.wrappers.AtariPreprocessing(test_env, frame_skip=4, grayscale_obs=True, scale_obs=True)\n",
    "                test_env = gym.wrappers.FrameStackObservation(test_env, stack_size=4)\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying Atari wrappers: {e}\")\n",
    "                continue\n",
    "        \n",
    "        hidden_dim = 128\n",
    "        \n",
    "        # Initializing the shared actor-critic model\n",
    "        shared_ac_network = create_shared_network(test_env, hidden_dim)\n",
    "        \n",
    "        # Creating a dummy simulation for testing all the envrionments\n",
    "        batch_size = 2\n",
    "        if isinstance(test_env.observation_space, gym.spaces.Discrete):\n",
    "            dummy_states = torch.randint(0, test_env.observation_space.n, (batch_size,))\n",
    "        elif isinstance(test_env.observation_space, gym.spaces.Box):\n",
    "            dummy_states = torch.randn(batch_size, *test_env.observation_space.shape)\n",
    "        else:\n",
    "            dummy_states = None\n",
    "        \n",
    "        # Performing a forward pass through the shared actor-critic model\n",
    "        if dummy_states is not None:\n",
    "            shared_newtork_output = shared_ac_network(dummy_states)\n",
    "            if isinstance(test_env.action_space, gym.spaces.Box):\n",
    "                (state_mean, log_std), state_value = shared_newtork_output\n",
    "                print(f\"Action Mean: {state_mean}, \\nAction Log Std: {log_std}, \\nState Value Estimates: {state_value}\")\n",
    "            else:\n",
    "                action_probs, state_value = shared_newtork_output\n",
    "                print(f\"Action Probabilities: {action_probs}, \\nState Value Estimates: {state_value}\")\n",
    "        \n",
    "        \n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd13f0b62b30ff",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee2dd81024ce246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The motivation behind using a general purpose shared network `create_shared_network(env)` is to design a flexible and scalable network architecture that can automatically handles different environments without needing manual adjustments. As we know Gym environments are very different in terms of what the observation space are and what type of actions are needed to be taken in those environments. The observations can be discrete numbers, continuous lists, or groups of image frames, whereas actions can be discrete choices, continuous actions, multi-discrete in nature. So this function dynamically looks at the types of spaces in these environments and then, it creates a shared actor-critic model that encodes these inputs accordingly and additionally select the right output head structure for either discrete or continuous actions. \n",
    "\n",
    "# The use of this kind of setup is particularly beneficial when the environments are hetrogeneous in nature - like CliffWalking-v0, which involves fixed states and actions, or LunarLander-v3, where you have box observations and fixed choices for actions. In PongNoFrameskip-v4, you work with image-based input and fixed actions, while in HalfCheetah-v5, you deal with continuous states and actions using MuJoCo, so instead of designing separate network architectures for each environments, we can use this efficient actor-critic model directly using this function. This architecture is highly useful in rprojects where experiments span various benchmarks, as it allows for rapid prototyping and testing of different algorithms without the need for extensive code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c886fa536a639",
   "metadata": {},
   "source": [
    "### Task 3: Write Observation Normalization Function\n",
    "Create a function `normalize_observation(obs, env)` that:\n",
    "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
    "- If so, normalize the input observation.\n",
    "- Otherwise, return the observation unchanged.\n",
    "\n",
    "```python\n",
    "# TODO: Define `normalize_observation(obs, env)`\n",
    "```\n",
    "\n",
    "Test this function with observations from:\n",
    "- `LunarLander-v3`\n",
    "- `PongNoFrameskip-v4`\n",
    "\n",
    "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc7ee06112cf7d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Testing for Environment: LunarLander-v3-----\n",
      "\n",
      "Initial Observation Space for LunarLander: \n",
      "[ 3.1213759e-04  1.4149177e+00  3.1597104e-02  1.7766991e-01\n",
      " -3.5485908e-04 -7.1572280e-03  0.0000000e+00  0.0000000e+00]\n",
      "Lunar observation range: -0.007157227955758572 to 1.4149177074432373\n",
      "\n",
      "Normalized Observation Space for LunarLander: \n",
      "[0.5000624  0.78298354 0.5015799  0.5088835  0.49997178 0.49964213\n",
      " 0.         0.        ]\n",
      "Lunar normalized observation range: 0.0 to 0.7829835414886475\n",
      "\n",
      "\n",
      "-----Testing for Environment: PongNoFrameskip-v4-----\n",
      "\n",
      "Initial Observation Space shape for Pong: (210, 160, 3)\n",
      "Pong observation pixel range: 0 to 228\n",
      "\n",
      "Normalized Observation Space shape for Pong:(210, 160, 3)\n",
      "Pong normalized observation pixel range: 0.0 to 0.8941176533699036\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_YOUR_CODE\n",
    "def normalize_observation(obs, env):\n",
    "    \n",
    "    # Check if the observation space is a Box Space and has low and high attributes\n",
    "    if isinstance(env.observation_space, gym.spaces.Box) and hasattr(env.observation_space, 'low') and hasattr(env.observation_space, 'high'):\n",
    "        low_attr = env.observation_space.low\n",
    "        high_attr = env.observation_space.high\n",
    "        \n",
    "        # For image based observations we take the low as 0 and high as 255, so divide by 255 to normalize\n",
    "        if np.all(low_attr == 0) and np.all(high_attr == 255):\n",
    "            obs = obs.astype(np.float32) / 255.0\n",
    "            return obs\n",
    "        # For other observations we normalize the values between 0 and 1\n",
    "        else:\n",
    "            difference_value = high_attr - low_attr\n",
    "            difference_value[difference_value == 0] = 1\n",
    "            obs = (obs - low_attr) / difference_value\n",
    "            return obs\n",
    "    else:\n",
    "        raise ValueError(\"Observation space is not a Box space or does not have low/high attributes.\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print(f\"\\n-----Testing for Environment: LunarLander-v3-----\\n\")\n",
    "    lunar_test_env = gym.make(\"LunarLander-v3\")\n",
    "    observation_lunar, _ = lunar_test_env.reset()\n",
    "    print(f\"Initial Observation Space for LunarLander: \\n{observation_lunar}\")\n",
    "    print(\"Lunar observation range: {} to {}\\n\".format(observation_lunar.min(), observation_lunar.max()))\n",
    "    normalized_observation_lunar = normalize_observation(observation_lunar, lunar_test_env)\n",
    "    print(f\"Normalized Observation Space for LunarLander: \\n{normalized_observation_lunar}\")\n",
    "    print(\"Lunar normalized observation range: {} to {}\\n\".format(normalized_observation_lunar.min(), normalized_observation_lunar.max()))\n",
    "    \n",
    "    print(f\"\\n-----Testing for Environment: PongNoFrameskip-v4-----\\n\")\n",
    "    pong_test_env = gym.make(\"PongNoFrameskip-v4\")\n",
    "    observation_pong, _ = pong_test_env.reset()\n",
    "    \n",
    "    if hasattr(pong_test_env, '__array__'):\n",
    "        observation_pong = np.array(observation_pong)\n",
    "    \n",
    "    print(f\"Initial Observation Space shape for Pong: {observation_pong.shape}\")\n",
    "    print(\"Pong observation pixel range: {} to {}\\n\".format(observation_pong.min(), observation_pong.max()))\n",
    "    normalized_observation_pong = normalize_observation(observation_pong, pong_test_env)\n",
    "    print(f\"Normalized Observation Space shape for Pong:{normalized_observation_pong.shape}\")\n",
    "    print(\"Pong normalized observation pixel range: {} to {}\".format(normalized_observation_pong.min(), normalized_observation_pong.max()))\n",
    "            \n",
    "       \n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ed2a6e7ca7a7b",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78211b617a843f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The motivation behind Normalization of observation is a crucial step in the preprocessing pipeline of reinforcement learning, especially when training deep neural networks. Raw observations can vary highly in size and value, which can make training unstable, and cause issues like disappearing or excessively large gradients, and exponentially slow down learning. The \"normalize_observation\" function helps by adjusting all these observations to fit within the same range, i.e [0,1], so that the changes in the observations are more consistent and easier to learn from. This method ensures that the neural network receives inputs that are all on a consistent scale and motivate the network to learn more easily give better outputs for the actor and critic heads.\n",
    "\n",
    "# Normalization is highly used when the environments have continuous observation spaces or when dealing with raw pixel inputs. For example, LunarLander-v3, uses an 8-dimensional state vector, which can have high numbers for position or velocity. By normalizing these numbers, none of them become too dominant, which helps the training process stay stable. While, environment like PongNoFrameskip-v4 consists of images that are 210 X 160 pixels which are RGB in nature so the pixel values will range from 0 to 255 and by scaling these values to a range between 0.0 and 1.0, the changes in obseervation becomes more stable and suitable for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fb5353307f514",
   "metadata": {},
   "source": [
    "## Section 4: Gradient Clipping\n",
    "\n",
    "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates.\n",
    "\n",
    "### Task 4: Clip Gradients for Actor-Critic Networks\n",
    "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
    "```python\n",
    "# During training, after loss.backward():\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "```\n",
    "\n",
    "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
    "Print the gradient norm before and after clipping to verify itâ€™s applied.\n",
    "\n",
    "ðŸ”— PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7327507fb6e803ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Loss: -0.41626057028770447\n",
      "Critic Loss: 1.5845913887023926\n",
      "Combined Loss: 1.1683307886123657\n",
      "Gradient Norm before optimization: 3.8339337676779257\n",
      "Gradient Norm after optimization: 0.4999999033364367\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "# Creating a Seperate Actor-Critic model as per 1a\n",
    "class SeparateActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(SeparateActorCritic, self).__init__()\n",
    "        \n",
    "        # Actor network\n",
    "        self.a_fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.a_fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "        self.a_softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Critic network\n",
    "        self.c_fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.c_fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # Forward pass for actor\n",
    "        f_actor = F.relu(self.a_fc1(state))\n",
    "        logits_actor = self.a_fc2(f_actor)\n",
    "        action_probs = self.a_softmax(logits_actor)\n",
    "        \n",
    "        # Forward pass for critic\n",
    "        f_critic = F.relu(self.c_fc1(state))\n",
    "        state_value = self.c_fc2(f_critic)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "    \n",
    "# Computing L2 norm of the gradients of all parameters in the model\n",
    "def get_gradient_normalization(model):\n",
    "    total = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            normalized_norm = param.grad.data.norm(2)\n",
    "            total += normalized_norm.item() ** 2\n",
    "    total = total ** 0.5\n",
    "    return total\n",
    "\n",
    "# Running a dummy simulation to test the gradient normalization\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    batch_size = 5\n",
    "    state_dim = 4\n",
    "    action_dim = 2\n",
    "    hidden_dim = 128\n",
    "    dummy_states = torch.randn(batch_size, state_dim)\n",
    "    \n",
    "    # Initializing the actor-critic model\n",
    "    AC_model = SeparateActorCritic(state_dim, action_dim, hidden_dim)\n",
    "    \n",
    "    # Forward pass through the model to get the action probabilities and state values\n",
    "    action_probs, state_values = AC_model(dummy_states)\n",
    "    \n",
    "    # For simulation of loss computation, by using multinomial sampling we sampled an action for each state\n",
    "    dummy_actions = action_probs.multinomial(num_samples=1).squeeze(-1)\n",
    "    \n",
    "    # Computing the log probabilities of the dummy actions taken\n",
    "    dummy_log_probs = torch.log(action_probs[range(batch_size), dummy_actions])\n",
    "    \n",
    "    # Computing the entropy of the each action probability distribution\n",
    "    dummy_entropy_value = -torch.sum(action_probs * torch.log(action_probs + 1e-10), dim=1)\n",
    "    \n",
    "    # Creating dummy rewards (or target values) for the critic evaluation\n",
    "    dummy_critic_rewards = torch.randn(batch_size)\n",
    "    \n",
    "    # Computing the advantage estimates which is the difference between the returns and the estimated state values\n",
    "    compute_advantage = dummy_critic_rewards - state_values.squeeze(-1)\n",
    "    \n",
    "    # Computing the actor loss using the max log probabilities and the advantage estimates\n",
    "    actor_loss = -torch.mean(dummy_log_probs * compute_advantage) - 0.01 * torch.mean(dummy_entropy_value)\n",
    "    \n",
    "    # Computing the critic loss using the mean squared error between the returns and the estimated state values\n",
    "    critic_loss = F.mse_loss(state_values.squeeze(-1), dummy_critic_rewards)\n",
    "    \n",
    "    print(\"Actor Loss:\", actor_loss.item())\n",
    "    print(\"Critic Loss:\", critic_loss.item())\n",
    "    total_loss = actor_loss + critic_loss\n",
    "    print(\"Combined Loss:\", total_loss.item())\n",
    "    \n",
    "    # Performing a backward pass and optimizing the model using the combined loss\n",
    "    optimizer = optim.Adam(AC_model.parameters(), lr=0.001)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Computing the gradient normalization\n",
    "    previous_norm = get_gradient_normalization(AC_model)\n",
    "    print(\"Gradient Norm before optimization:\", previous_norm)\n",
    "    \n",
    "    # Performing the gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(AC_model.parameters(), max_norm=0.5)\n",
    "    \n",
    "    # Computing the gradient normalization after clipping\n",
    "    clipped_norm = get_gradient_normalization(AC_model)\n",
    "    print(\"Gradient Norm after optimization:\", clipped_norm)\n",
    "    \n",
    "    # Performing the optimization step\n",
    "    optimizer.step()\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952750fa74cd487",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557a9303f5a1c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The motivation behind using Gradient clipping is to prevent the exploding gradient issue during backpropogation, as in the case of actor-critic methods, the optimization for both the actor and critic is performed simultaneously, which can lead to large gradients due to high variance environments or when the rewards has super rapid highs or lows, which ultimately can destabilize the training process. So by applyin L2-Norm clipping we can limit the maximum allowed gradient norm to 0.5 as used above, ensuring that the updates remain within a reasonable and controlled range, and allow the model to have smoother, more stable learning.\n",
    "\n",
    "# Gradient clipping is particularly beneficial in environments with variable or big rewards. For example, Atari environment like BipedalWalker-v3, where rewards heavily depend on prior exploration, and during RNN training where gradients may explode or vanish. In the dummy simulation used above, gradient clipping is being applied on a typical separate actor-critic neural network, and  effect in the  gradient norms before and after (clipping) can be clearly observed. This is routinely done with optimizers like Adam, or RMS, which allow for fast convergence with high numerical stability in various RL tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cff31e6c6e7e4a",
   "metadata": {},
   "source": [
    "If you are working in a team, provide a contribution summary.\n",
    "| Team Member | Step# | Contribution (%) |\n",
    "|---|---|---|\n",
    "|  50604127 (sgupta67), 506010533 (karamchan) | Task 1 |  80%, 20% |\n",
    "|  50604127 (sgupta67) | Task 2 |  100% |\n",
    "|  506010533 (karamchan) | Task 3 | 100%  |\n",
    "| 50604127 (sgupta67), 506010533 (karamchan)  | Task 4 | 20%, 80%  |\n",
    "|   | **Total** | 100%  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a1a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
